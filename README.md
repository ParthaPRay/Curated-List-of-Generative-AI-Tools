# Curated-List-of-Generative-AI-Tools
This repo contains the curated list of tools for generative AI




# Models

Large-scale language models (LLMs) are distinguished by their comprehensive language comprehension and generation abilities. These models are trained on vast data sets, learning billions of parameters, and require significant computational power for both training and operation. Typically structured as artificial neural networks, predominantly transformers, LLMs are trained through self-supervised and semi-supervised learning methods.

Functioning as autoregressive language models, LLMs process input text and iteratively predict subsequent words or tokens. Until 2020, fine-tuning was the sole approach for tailoring these models to specific tasks. However, larger models like GPT-3 have demonstrated that prompt engineering can achieve comparable results. LLMs are believed to assimilate knowledge of syntax, semantics, and "ontology" from human language data, but they also inherit any inaccuracies and biases present in these data sources.

Prominent examples of LLMs include OpenAI's GPT series (such as GPT-3.5 and GPT-4 used in ChatGPT), Google's PaLM (utilized in Bard), Meta's LLaMA, along with BLOOM, Ernie 3.0 Titan, and Anthropic's Claude 2.

We present the comparative list of LLMs below. Traning cost is presented as (petaFLOP/day). For the training cost column, 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP.


| Model Name                    | Release Year   | Developer                     | #Parameters  | Corpus size                                                      | Training cost  | License                 | Comments                                                                                                                                                                                |
|-------------------------|-------------------|-------------------------------|--------------------------|------------------------------------------------------------------|------------------------------|-----------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| GPT-1                   | Jun-18            | OpenAI                        | 117 million              |                                                                  |                              |                             | First GPT model, decoder-only transformer                                                                                                                                             |
| BERT                    | Oct-18            | Google                        | 340 million        | 3.3 billion words                                          | 9                      | Apache 2.0             | An early and influential language model, but encoder-only and thus not built to be prompted or generative                                                                   |
| XLNet                   | Jun-19            | Google                        | ~340 million        | 33 billion words                                                |                              |                             | An alternative to BERT; designed as encoder-only                                                                                                                             |
| GPT-2                   | Feb-19            | OpenAI                        | 1.5 billion       | 40GB (~10 billion tokens)                              |                              | MIT                   | general-purpose model based on transformer architecture                                                                                                                                |
| GPT-3                   | May-20            | OpenAI                        | 175 billion         | 300 billion tokens                                        | 3640                    | proprietary                 | A fine-tuned variant of GPT-3, termed GPT-3.5, was made available to the public through a web interface called ChatGPT in 2022                                                   |
| GPT-Neo                 | Mar-21            | EleutherAI                    | 2.7 billion         | 825 GiB                                                   |                              | MIT                    | The first of a series of free GPT-3 alternatives released by EleutherAI. GPT-Neo outperformed an equivalent-size GPT-3 model on some benchmarks, but was significantly worse than the largest GPT-3 |
| GPT-J                   | Jun-21            | EleutherAI                    | 6 billion           | 825 GiB                                                  | 200                    | Apache 2.0                  | GPT-3-style language model                                                                                                                                                             |
| Megatron-Turing NLG     | October 2021 | Microsoft and Nvidia          | 530 billion        | 338.6 billion tokens                                      |                              | Restricted web access       | Standard architecture but trained on a supercomputing cluster                                                                                                                        |
| Ernie 3.0 Titan         | Dec-21            | Baidu                         | 260 billion         | 4 Tb                                                            |                              | Proprietary                 | Chinese-language LLM. Ernie Bot is based on this model                                                                                                                                |
| Claude             | Dec-21            | Anthropic                     | 52 billion        | 400 billion tokens                                        |                              | beta                        | Fine-tuned for desirable behavior in conversations                                                                                                                               |
| GLaM (Generalist Language Model)                   | Dec-21            | Google                        | 1.2 trillion       | 1.6 trillion tokens                                         | 5600                     | Proprietary                 | Sparse mixture of experts model, making it more expensive to train but cheaper to run inference compared to GPT-3                                                                     |
| Gopher                  | Dec-21            | DeepMind                      | 280 billion        | 300 billion tokens                                         | 5833                    | Proprietary                 | Further developed into the Chinchilla model                                                                                                                                           |
| LaMDA (Language Models for Dialog Applications)                  | Jan-22            | Google                        | 137 billion        | 1.56T words, 168 billion tokens                        | 4110                   | Proprietary                 | Specialized for response generation in conversations                                                                                                                                 |
| GPT-NeoX                | Feb-22            | EleutherAI                    | 20 billion         | 825 GiB                                                    | 740                     | Apache 2.0                  | based on the Megatron architecture                                                                                                                                                      |
| Chinchilla              | Mar-22            | DeepMind                      | 70 billion        | 1.4 trillion tokens                                   | 6805                  | Proprietary                 | Reduced-parameter model trained on more data. Used in the Sparrow bot. Often cited for its neural scaling law                                                                         |
| PaLM (Pathways Language Model)                   | Apr-22            | Google                        | 540 billion         | 768 billion tokens                                         | 29250                  | Proprietary                 | aimed to reach the practical limits of model scale                                                                                                                                     |
| OPT (Open Pretrained Transformer)                    | May-22            | Meta                          | 175 billion        | 180 billion tokens                                        | 310                    | Non-commercial research  | GPT-3 architecture with some adaptations from Megatron                                                                                                                                 |
| YaLM 100B               | Jun-22            | Yandex                        | 100 billion         | 1.7TB                                                      |                              | Apache 2.0                  | English-Russian model based on Microsoft's Megatron-LM                                                                                                                                |
| Minerva                 | Jun-22            | Google                        | 540 billion        | 38.5B tokens from webpages filtered for mathematical content and from papers submitted to the arXiv preprint server |                              | Proprietary                 | LLM trained for solving "mathematical and scientific questions using step-by-step reasoning". Minerva is based on PaLM model, further trained on mathematical and scientific data |
| BLOOM                   | Jul-22            | Large collaboration led by Hugging Face | 175 billion         | 350 billion tokens (1.6TB)                                  |                              | Responsible AI              | Essentially GPT-3 but trained on a multi-lingual corpus (30% English excluding programming languages)                                                                                  |
| Galactica               | Nov-22            | Meta                          | 120 billion              | 106 billion tokens                                        | unknown                       | CC-BY-NC-4.0                | Trained on scientific text and modalities                                                                                                                                             |
| AlexaTM (Teacher Models)                | Nov-22            | Amazon                        | 20 billion          | 1.3 trillion                                               |                              | proprietary           | bidirectional sequence-to-sequence architecture                                                                                                                                       |
| LLaMA (Large Language Model Meta AI)                  | Feb-23            | Meta                          | 65 billion          | 1.4 trillion                                               | 6300                    | Non-commercial research  | Trained on a large 20-language corpus to aim for better performance with fewer parameters. Researchers from Stanford University trained a fine-tuned model based on LLaMA weights, called Alpaca |
| GPT-4                   | Mar-23            | OpenAI                        | Exact number unknown   | Unknown                                                         | Unknown                       | proprietary                 | Available for ChatGPT Plus users and used in several products                                                                                                                         |
| Cerebras-GPT            | Mar-23            | Cerebras                      | 13 billion         |                                                                  | 270                      | Apache 2.0                  | Trained with Chinchilla formula                                                                                                                                                      |
| Falcon                  | Mar-23            | Technology Innovation Institute | 40 billion          | 1 trillion tokens, from RefinedWeb (filtered web text corpus) plus some "curated corpora" | 2800                   | Apache 2.0             |                                                                                                                                                                                        |
| BloombergGPT            | Mar-23            | Bloomberg L.P.                | 50 billion               | 363 billion token dataset based on Bloomberg's data sources, plus 345 billion tokens from general purpose datasets  |                              | Proprietary                 | LLM trained on financial data from proprietary sources, that "outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks" |
| PanGu-Σ                 | Mar-23            | Huawei                        | 1.085 trillion            | 329 billion tokens                                        |                              | Proprietary                 |                                                                                                                                                                                        |
| OpenAssistant      | Mar-23            | LAION                         | 17 billion               | 1.5 trillion tokens                                              |                              | Apache 2.0                  | Trained on crowdsourced open data                                                                                                                                                       |
| Jurassic-2         | Mar-23            | AI21 Labs                     | Exact size unknown        | Unknown                                                         |                              | Proprietary                 | Multilingual                                                                                                                                                                     |
| PaLM 2                  | May-23            | Google                        | 340 billion         | 3.6 trillion tokens                                        | 85000                   | Proprietary                 | Used in Bard chatbot                                                                                                                                                           |
| Llama 2               | Jul-23       | Meta                                | 70 billion       | 2 trillion tokens |                              | Llama 2 license     | Successor of LLaMA                                                                                                                                                                                                                 |
| Claude 2              | Jul-23       | Anthropic                           | Unknown                 | Unknown                | Unknown                       | Proprietary         | Used in Claude chatbot                                                                                                                                                                                                       |
| Falcon 180B           | Sep-23       | Technology Innovation Institute     | 180 billion       | 3.5 trillion tokens|                              | Falcon 180B TII license |                                                                                                                                                                                                                                    |
| Mistral 7B            | Sep-23       | Mistral AI                          | 7.3 billion       | Unknown                |                              | Apache 2.0          |                                                                                                                                                                                                                                    |
| OpenHermes-15B  | Sep-23       | Nous Research                       | 13 billion        | Unknown                | Unknown                       | MIT                 |                                                                                                                                                                                                                                    |
| Claude 2.1            | Nov-23       | Anthropic                           | Unknown                 | Unknown                | Unknown                       | Proprietary         | Used in Claude chatbot. Has a context window of 200,000 tokens, or ~500 pages                                                                                                                                                |
| Grok-1                | Nov-23       | x.AI                                | Unknown                 | Unknown                | Unknown                       | Proprietary         | Used in Grok chatbot. Grok-1 has a context length of 8,192 tokens and has access to X (Twitter)                                                                                                                               |
| Gemini                | Dec-23       | Google DeepMind                     | Unknown                 | Unknown                | Unknown                       | Proprietary         | Multimodal model, comes in three sizes. Used in Bard chatbot                                                                                                                                                               |
| Mixtral 8x7B          | Dec-23       | Mistral AI                          | 46.7B total, 12.9B parameters per token | Unknown | Unknown                       | Apache 2.0          | Mixture of experts model, outperforms GPT-3.5 and Llama 2 70B on many benchmarks. All weights were released via torrent                                                                                                      |
| Phi-2                 | Dec-23       | Microsoft                           | 2.7B                    | 1.4T tokens             | Unknown                       | Proprietary         | So-called small language model, that "matches or outperforms models up to 25x larger", trained on "textbook-quality" data based on the paper "Textbooks Are All You Need". Model training took "14 days on 96 A100 GPUs"     |




# Developer Tools

1. 


# Chatbots

1. ChatGPT - ChatGPT by OpenAI is a large language model that interacts in a conversational way.
2. Bing Chat - A conversational AI language model powered by Microsoft Bing.
3. Bard - An experimental AI chatbot by Google, powered by the LaMDA model.
4. Character.AI - Character.AI lets you create characters and chat to them.
5. ChatPDF - Chat with any PDF.
5. ChatSonic - An AI-powered assistant that enables text and image creation.


# References

1. https://en.wikipedia.org/wiki/Generative_artificial_intelligence
2. https://en.wikipedia.org/wiki/Large_language_model
3. https://github.com/steven2358/awesome-generative-ai
4. https://www.turing.com/resources/generative-ai-tools
5. https://aimagazine.com/top10/top-10-generative-ai-tools
